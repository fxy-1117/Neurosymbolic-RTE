{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk,re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from amr_logic_converter import types\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from word_forms.word_forms import get_word_forms\n",
    "from sympy.logic.boolalg import to_cnf\n",
    "from sympy.abc import A, B,C\n",
    "import sympy\n",
    "from sklearn.metrics import classification_report\n",
    "from sympy import Symbol,simplify_logic\n",
    "from pysat.formula import CNF\n",
    "from pysat.solvers import Solver\n",
    "from pattern.en import conjugate, lemma, lexeme, PRESENT, SG\n",
    "import inflect\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transition_amr_parser.parse import AMRParser\n",
    "from amr_logic_converter import AmrLogicConverter\n",
    "from transformers import pipeline\n",
    "# Download and save a model named AMR3.0 to cache\n",
    "parser = AMRParser.from_pretrained('AMR3-joint-ontowiki-seed43')\n",
    "converter = AmrLogicConverter(existentially_quantify_instances=False,invert_relations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate PL\n",
    "def generate_logic(data):\n",
    "    tem  = []\n",
    "    temm = []\n",
    "    tem_token = []\n",
    "    for sen in data:\n",
    "        tokens, positions = parser.tokenize(sen)\n",
    "        tem_token.append(tokens)\n",
    "    \n",
    "    annotations, machines = parser.parse_sentences(tem_token)\n",
    "    tem = annotations\n",
    "    temm = [i.get_amr().to_penman(jamr=False, isi=False) for i in machines]\n",
    "    n = 0\n",
    "    r1 = []\n",
    "    r2 = []\n",
    "    for sen in data:\n",
    "        r1.append(converter.convert(tem[n]))\n",
    "        r2.append(converter.convert(temm[n]))\n",
    "        n+=1\n",
    "    return tem,temm, r1,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe299178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get substring for dyadic predicate\n",
    "def get_substring(s,w1,w2):\n",
    "    p = inflect.engine()\n",
    "#     print(s,w1,w2)\n",
    "    if w1.isnumeric():\n",
    "        w1 = p.number_to_words(w1)\n",
    "    if w2.isnumeric():\n",
    "        w2 = p.number_to_words(w2) \n",
    "    w11 = []\n",
    "    if w1==\"be-located-at\":\n",
    "        w11 = [\"on\",\"at\",\"in\"]\n",
    "    w22 = []\n",
    "    if w2==\"be-located-at\":\n",
    "        w22 = [\"on\",\"at\",\"in\"]\n",
    "    \n",
    "    w111 = []\n",
    "    if w1 == \"person\":\n",
    "        w111 = [str(tok) for tok in nlp(s) if (tok.dep_ == \"nsubj\")]\n",
    "    w222 = []\n",
    "    if w2== \"person\":\n",
    "        w222 = [str(tok) for tok in nlp(s) if (tok.dep_ == \"nsubj\")]\n",
    "\n",
    "\n",
    "    sub1 = [j for i in get_word_forms(w1,0.7) for j in get_word_forms(w1,0.7)[i]]+[w1]+w11+w111\n",
    "    sub2 = [j for i in get_word_forms(w2,0.7) for j in get_word_forms(w2,0.7)[i]]+[w2]+w22+w222\n",
    "    search1 = 0\n",
    "    search2 = 999\n",
    "    c1= 0\n",
    "    c2= 0\n",
    "\n",
    "    token = word_tokenize(s.lower())\n",
    "#     print(token)\n",
    "    for i in range(len(token)):\n",
    "        if token[i] in sub1:\n",
    "            c1 = 1\n",
    "            search1 = i\n",
    "        elif token[i] in sub2:\n",
    "            c2 = 1\n",
    "            search2 = i\n",
    "#     print(search1,search2)\n",
    "# print(search)\n",
    "    if c1 == 0 or c2 == 0:\n",
    "         return False\n",
    "\n",
    "    if search1 > search2:\n",
    "#         print(\"The extracted string : \" + \" \".join(token[search2:search1+1]))\n",
    "        return \" \".join(token[search2:search1+1])\n",
    "    else:\n",
    "#         print(\"The extracted string : \" + \" \".join(token[search1:search2+1]))\n",
    "        return \" \".join(token[search1:search2+1])\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46210d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pysat formula\n",
    "def combine(final,f = False):\n",
    "    init = True\n",
    "    for i in final:\n",
    "        if type(i) == list:\n",
    "            tem = True\n",
    "            \n",
    "            tem = tem&combine(i)\n",
    "            if ~tem == -1:\n",
    "                \n",
    "                init = init & True\n",
    "            elif ~tem == -2:\n",
    "                if not f:\n",
    "                    init = init & False\n",
    "                else:\n",
    "                    init = init & True\n",
    "            else:\n",
    "                init = init&~tem\n",
    "        else:\n",
    "            init = init&i\n",
    "            \n",
    "    return init\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188461ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# convert to pysat formula\n",
    "def transform(formula,Var,X):\n",
    "    final = copy.deepcopy(formula)\n",
    "    for i in range(len(final)):\n",
    "        \n",
    "        if type(final[i]) == list:\n",
    "            \n",
    "            if final[i][0] == \"ARG\":\n",
    "                if \" \".join([Var[final[i][1]],Var[final[i][2]],final[i][3]]) not in X:\n",
    "                    continue\n",
    "                else:\n",
    "                    final[i] = X[\" \".join([Var[final[i][1]],Var[final[i][2]],final[i][3]])]\n",
    "#             \n",
    "                \n",
    "#                     init = init\n",
    "            else:\n",
    "                final[i] = transform(final[i],Var,X)\n",
    "               \n",
    "#                     else:\n",
    "                    \n",
    "        else:\n",
    "            if final[i] not in X:\n",
    "                continue\n",
    "        \n",
    "            else:\n",
    "                final[i]  = X[final[i]] \n",
    "  \n",
    "    return final\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract predicates from PL \n",
    "def extract(formula):\n",
    "    and_list = []\n",
    "    var = {}\n",
    "    arg = []\n",
    "    if type(formula) == types.Not:\n",
    "        return [extract(formula.body)[0]],{**extract(formula.body)[1],**var},arg+extract(formula.body)[2]\n",
    "    for i in formula.args:\n",
    "        if type(i) == types.Not:\n",
    "            and_list.append(extract(i.body)[0])\n",
    "            var = {**extract(i.body)[1],**var}\n",
    "            arg = arg+extract(i.body)[2]\n",
    "           \n",
    "        else:\n",
    "            if i.predicate.symbol[0] ==\":\":\n",
    "#             if i.predicate.symbol[0] ==\"รท:\"\n",
    "                and_list.append([\"ARG\"]+ [i.terms[j].value for j in range(0,len(i.terms))]+[i.predicate.symbol])\n",
    "                arg.append([i.terms[j].value for j in range(0,len(i.terms))]+[i.predicate.symbol])\n",
    "            else:\n",
    "                and_list.append(re.sub(r'\\-*[0-9]',\"\",i.predicate.symbol))\n",
    "                var[i.terms[0].value] = re.sub(r'\\-*[0-9]',\"\",i.predicate.symbol)\n",
    "    return and_list,var,arg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sentences/words similarity\n",
    "def score(s1,s2):\n",
    "    sentences = [s1,s2]\n",
    "    embedding_1= model.encode(sentences[0], convert_to_tensor=True,show_progress_bar=False)\n",
    "    embedding_2 = model.encode(sentences[1], convert_to_tensor=True,show_progress_bar=False)\n",
    "    return util.pytorch_cos_sim(embedding_1, embedding_2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c971903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate PYSAT formula\n",
    "def pysat_formula(formula):\n",
    "    tem_list = []\n",
    "    for i in str(formula).split(\" & \"):\n",
    "        if i[0] == \"x\":\n",
    "            tem_list.append([int(i[1:])])\n",
    "        else:\n",
    "            tem_tem = []\n",
    "            for j in i.replace(\"(\",\"\").replace(\")\",\"\").split(\" | \"):\n",
    "                if j[0] == \"~\":\n",
    "#                 print(int(j[-1]))\n",
    "                    tem_tem.append(int(j[2:])*-1)\n",
    "                elif j[0] == \"x\":\n",
    "                    tem_tem.append(int(j[1:]))\n",
    "            tem_list.append(tem_tem)\n",
    "    return tem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsitute variable according similarity\n",
    "def subsitute(x,y,replaceX,replaceXX,maxx,i,j,thre):\n",
    "    tems = score(x,y)\n",
    "                                    \n",
    "    if tems>=thre:\n",
    "        if tems > maxx[i]:\n",
    "            maxx[i] = tems\n",
    "            replaceXX[i] = replaceX[j]\n",
    "            return True\n",
    "    return False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relaxed proposition entailment/contradiction to classify RTE dataset\n",
    "def prove(data,sent):\n",
    "    checkArg0 = []\n",
    "    checkVaribale0 = {}\n",
    "    # extract predicate from premise\n",
    "    for0,checkVaribale0,checkArg0 = extract(data[0])\n",
    "    for i in checkArg0:\n",
    "        for j in range(len(i)):\n",
    "#             print(j)\n",
    "            if i[j] in checkVaribale0:\n",
    "                i[j] = checkVaribale0[i[j]]\n",
    "            else:\n",
    "                checkVaribale0[i[j]] = i[j]\n",
    "\n",
    "    replaceX = {}\n",
    "    n = 1\n",
    "    for i in checkVaribale0:\n",
    "        if checkVaribale0[i][0] == \":\":\n",
    "            continue\n",
    "        if checkVaribale0[i] not in replaceX:\n",
    "            replaceX[checkVaribale0[i]] = Symbol('x'+str(n))  \n",
    "            n+=1\n",
    "    for i in checkArg0:\n",
    "#         print(i)\n",
    "        if \" \".join(i) not in replaceX:\n",
    "            replaceX[\" \".join(i)] = Symbol('x'+str(n))  \n",
    "            n+=1\n",
    "    explanation = False\n",
    "    if len(data)>2:\n",
    "        explanation = True\n",
    "    if explanation:\n",
    "        checkArg2 = []\n",
    "        checkVaribale2 ={}\n",
    "        # extract predicate from explanation\n",
    "        for2,checkVaribale2,checkArg2 = extract(data[2])\n",
    "        for i in checkArg2:\n",
    "            for j in range(len(i)):\n",
    "#             print(j)\n",
    "                if i[j] in checkVaribale2:\n",
    "                    i[j] = checkVaribale2[i[j]]\n",
    "                else:\n",
    "                    checkVaribale2[i[j]] = i[j]\n",
    "    \n",
    "#         print(checkVaribale2)\n",
    "        for i in checkVaribale2:\n",
    "#             print(i)\n",
    "            if checkVaribale2[i][0] == \":\":\n",
    "                continue\n",
    "            if checkVaribale2[i] not in replaceX:\n",
    "                replaceX[checkVaribale2[i]] = Symbol('x'+str(n))  \n",
    "                n+=1\n",
    "        for i in checkArg2:\n",
    "            if \" \".join(i) not in replaceX:\n",
    "                replaceX[\" \".join(i)] = Symbol('x'+str(n))  \n",
    "                n+=1\n",
    "    checkArg11 = []\n",
    "    checkVaribale11 ={}\n",
    "    # extract predicate from claim\n",
    "    for1,checkVaribale11,checkArg11 = extract(data[1])\n",
    "    \n",
    "    replaceXX = {}\n",
    "\n",
    "    thre = 0.55\n",
    "    for i in checkArg11:\n",
    "        for j in range(len(i)):\n",
    "            if i[j] in checkVaribale11:\n",
    "                i[j] = checkVaribale11[i[j]]\n",
    "            else:\n",
    "#                 else:\n",
    "                checkVaribale11[i[j]] = i[j]\n",
    "    maxx = {}\n",
    "    for i in checkArg11:\n",
    "        maxx[\" \".join(i)] = 0\n",
    "    for i in checkVaribale11:\n",
    "        maxx[checkVaribale11[i]] = 0\n",
    "    temj = \"\"\n",
    "    for i in checkVaribale11:\n",
    "        if checkVaribale11[i][0] == \":\":\n",
    "            continue\n",
    "        if checkVaribale11[i] in replaceX:\n",
    "            replaceXX[checkVaribale11[i]] = replaceX[checkVaribale11[i]]\n",
    "\n",
    "        else:\n",
    "            ccccc = 0\n",
    "            for j in replaceX:\n",
    "                if len(j.split())>1:\n",
    "                    continue\n",
    "\n",
    "                subsitute(checkVaribale11[i],j,replaceX,replaceXX,maxx,checkVaribale11[i],j,thre)\n",
    "                \n",
    "            if maxx[checkVaribale11[i]] == 0:\n",
    "                replaceXX[checkVaribale11[i]] =Symbol('x'+str(n))\n",
    "#                 formula11 = &formula11\n",
    "                \n",
    "                n+=1\n",
    "    for i in checkArg11:\n",
    "        if \" \".join(i) in replaceXX:\n",
    "            continue\n",
    "#         print(i)\n",
    "        if \" \".join(i) in replaceX:\n",
    "#             print(i)\n",
    "            replaceXX[\" \".join(i)] = replaceX[\" \".join(i)]\n",
    "\n",
    "        else:\n",
    "            temj = \" \"\n",
    "            for j in replaceX:\n",
    "                \n",
    "#                 print(j)\n",
    "                if len(j.split())<3:\n",
    "#                     continue\n",
    "                    if True:\n",
    "#                         print(j,i,temj)\n",
    "                        \n",
    "                        if subsitute(j,\" \".join([i[0],i[-2]]),replaceX,replaceXX,maxx,\" \".join(i),j,thre):\n",
    "                            temj = j\n",
    "                               \n",
    "                \n",
    "                else:\n",
    "\n",
    "                            tems3 = False\n",
    "                            tems1 = get_substring(sent[1],i[0],i[-2])\n",
    "                            tems2 = get_substring(sent[0],j.split()[0],j.split()[-2])\n",
    "                            if explanation:\n",
    "                                tems3 = get_substring(sent[2],j.split()[0],j.split()[-2])\n",
    "#                            \n",
    "                            if not tems1:\n",
    "                                if tems2:\n",
    "                                    if subsitute(\" \".join([i[0],i[-2]]),tems2,replaceX,replaceXX,maxx,\" \".join(i),j,thre,):\n",
    "                                        temj = j\n",
    "    \n",
    "                                if tems3:\n",
    "                                    if subsitute(\" \".join([i[0],i[-2]]),tems3,replaceX,replaceXX,maxx,\" \".join(i),j,thre):\n",
    "                                        temj = j\n",
    "                                   \n",
    "                                if subsitute(\" \".join([i[0],i[-2]]),\" \".join([j.split()[0],j.split()[1]]),replaceX,replaceXX,maxx,\" \".join(i),j,thre,):\n",
    "                                    temj = j\n",
    "                           \n",
    "                                        \n",
    "                            if tems2 and tems1:\n",
    "                                if subsitute(tems1,tems2,replaceX,replaceXX,maxx,\" \".join(i),j,thre,):\n",
    "                                    temj = j\n",
    "                                \n",
    "                        \n",
    "                            if tems3 and tems1:\n",
    "                                if subsitute(tems1,tems3,replaceX,replaceXX,maxx,\" \".join(i),j,thre,):\n",
    "                                    temj = j\n",
    "                   \n",
    "            if maxx[\" \".join(i)] == 0:\n",
    "                replaceXX[\" \".join(i)] = Symbol('x'+str(n))\n",
    "                n+=1\n",
    "            else:\n",
    "               \n",
    "                if i[0] in replaceXX:\n",
    "                    replaceXX[i[0]] = True\n",
    "                if i[-2] in replaceXX:\n",
    "                    replaceXX[i[-2]]= True\n",
    "    new_rex = {}\n",
    "    for i in replaceXX:\n",
    "        if i == \"and\":\n",
    "            new_rex[i] = True\n",
    "        if i.split()[0]==\"and\" and i.split()[-1][:3]==\":op\":\n",
    "            new_rex[i] = new_rex[i.split()[1]]\n",
    "        else:\n",
    "            new_rex[i] = replaceXX[i]\n",
    "    new_re = {}\n",
    "#     print(for2)\n",
    "    for i in replaceX:\n",
    "#         print(replaceXX.values())\n",
    "        tcc = 0\n",
    "        for j in new_rex:\n",
    "        \n",
    "            if isinstance(new_rex[j], sympy.Not):\n",
    "                if ~new_rex[j] == replaceX[i]:\n",
    "#                     print(new_rex[j] ,~new_rex[j] )\n",
    "                    new_re[i] = replaceX[i]\n",
    "                    tcc = 1\n",
    "            else:\n",
    "                if new_rex[j] == replaceX[i]:\n",
    "                    new_re[i] = replaceX[i]\n",
    "                    tcc = 1\n",
    "        if tcc == 0:\n",
    "                new_re[i] = True\n",
    "\n",
    "    formula0 = combine(transform(for0,checkVaribale0, replaceX))\n",
    "\n",
    "    formula11 =  combine(transform(for1,checkVaribale11, new_rex))\n",
    "    if formula11 == -1:\n",
    "        formula11 = True\n",
    "    elif formula11 == -2:\n",
    "        formula11 =False\n",
    "    elif formula11 == 0:\n",
    "        formula11 =False\n",
    "    elif formula11 == 1:\n",
    "        formula11 =True\n",
    "    if explanation:\n",
    "        formula2 = combine(transform(for2,checkVaribale2, replaceX))\n",
    "        final_formula = to_cnf( (formula0 & formula2 ) & ~(formula11))\n",
    "    else:\n",
    "        final_formula = to_cnf( formula0 & ~(formula11))\n",
    "    cnf = CNF(from_clauses=pysat_formula(final_formula))\n",
    "   \n",
    "    formula00 = combine(transform(for0,checkVaribale0, new_re))\n",
    "    \n",
    "    if explanation:\n",
    "        formula22 = combine(transform(for2,checkVaribale2, new_re))\n",
    "        if formula22 == -1:\n",
    "            formula22 = True\n",
    "        elif formula22 == -2:\n",
    "            formula22 = False\n",
    "        elif formula22 == 1:\n",
    "            formula22 = True\n",
    "        elif formula22 == 0:\n",
    "            formula22 = False\n",
    "        if formula00 == -1:\n",
    "            formula00 = True\n",
    "        elif formula00 == -2:\n",
    "            formula00 =False\n",
    "        elif formula00 == 1:\n",
    "            formula00 =True\n",
    "        elif formula00 == 0:\n",
    "            formula00 =False\n",
    "        final_formula11 = to_cnf( formula00 & formula22 & formula11)\n",
    "    else:\n",
    "        if formula00 == -1:\n",
    "            formula00 = True\n",
    "        elif formula00 == -2:\n",
    "            fomula00 = False\n",
    "\n",
    "        final_formula11 = to_cnf( formula00 & formula11)\n",
    "\n",
    "    cnf11 = CNF(from_clauses=pysat_formula(final_formula11))\n",
    "#\n",
    "    with Solver(name = \"Minisat22\",bootstrap_with=cnf) as solver:\n",
    "        \n",
    "        check_ent = solver.solve()\n",
    "\n",
    "    with Solver(name = \"Minisat22\",bootstrap_with=cnf11) as solver:\n",
    "        check_con1 = solver.solve()\n",
    "#     print(check_con)\n",
    "    # do classification\n",
    "    if not check_ent and check_con1:\n",
    "        return \"ent\"\n",
    "    elif not check_con1 and check_ent:\n",
    "\n",
    "        return \"con\"\n",
    "    \n",
    "    elif not check_con1 and not check_ent:\n",
    "        return \"both\"\n",
    "\n",
    "    else:\n",
    "        return \"neu\"\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2154aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb746241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-SNLI dataset\n",
    "df = pd.read_csv('esnli_train_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49781f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_ent = []\n",
    "sent_con = []\n",
    "sent_neu = []\n",
    "# set max length\n",
    "length = 2999999\n",
    "for i in range(0,259999):\n",
    "\n",
    "        try:\n",
    "            if len(word_tokenize(df.iloc[i,2]))>length or len(word_tokenize(df.iloc[i,3]))>length or len(word_tokenize(df.iloc[i,4]))>length:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if df.iloc[i,1] == \"entailment\":\n",
    "#             conti\n",
    "\n",
    "            sent_ent.append([df.iloc[i,2],df.iloc[i,3],df.iloc[i,4],'ent'])\n",
    "\n",
    "        elif df.iloc[i,1] == \"contradiction\":\n",
    "            sent_con.append([df.iloc[i,2],df.iloc[i,3],df.iloc[i,4],'con'])   \n",
    "        else:\n",
    "#             elif df.iloc[i,1] == \"contradicton\":\n",
    "            sent_neu.append([df.iloc[i,2],df.iloc[i,3],df.iloc[i,4],'neu']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "esnli_sent = [sent_ent[i] for i in np.random.choice(len(sent_ent), 50\n",
    "                                                    ,replace=False)]+[sent_neu[i] for i in np.random.choice(len(sent_neu), 50,replace=False)]+[sent_con[i] for i in np.random.choice(len(sent_con), 50,replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(esnli_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = []\n",
    "for i in tqdm(esnli_sent):\n",
    "#     try:\n",
    "    pre_data.append(generate_logic(i[:-1])[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b32465",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = []\n",
    "gl = []\n",
    "for i in tqdm(range(0,150)):\n",
    "    try:\n",
    "#     pre_data = (generate_logic(i[:-1])[-2])\n",
    "        tem = prove(pre_data[i][:],esnli_sent[i])\n",
    "#         print(tem)\n",
    "        if tem == \"both\":\n",
    "            print(\"both\")\n",
    "            continue\n",
    "        ll.append(tem)\n",
    "        gl.append(esnli_sent[i][-1])\n",
    "    except:\n",
    "        print(\"exception\")\n",
    "        continue\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7bccc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get classification/evaluation report\n",
    "testy = gl\n",
    "yhat_classes = ll\n",
    "\n",
    "accuracy = accuracy_score(testy, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "\n",
    "print(classification_report(testy, yhat_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145633c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(testy, yhat_classes,labels = [\"ent\",\"con\",\"neu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SICK dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = [load_dataset(\"sick\",split=\"train\"),load_dataset(\"sick\",split=\"test\"),load_dataset(\"sick\",split=\"validation\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e65096",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ent = []\n",
    "sent_con = []\n",
    "sent_neu = []\n",
    "length = 10\n",
    "# with open('sentences.txt', 'w') as f:\n",
    "for j in dataset:\n",
    "    for i in j:\n",
    "#         print(i)รท\n",
    "#         if df.iloc[i,1] != \"entailment\":\n",
    "#             continue\n",
    "        if len(word_tokenize(i[\"sentence_A\"]))>length or len(word_tokenize(i[\"sentence_A\"]))>length:\n",
    "            continue\n",
    "#         print(df.iloc[i,3])\n",
    "        \n",
    "#         print(generated_sentence)\n",
    "        if i[\"label\"] == 0:\n",
    "            if i[\"entailment_AB\"] == ' A_entails_B':\n",
    "#             conti\n",
    "                sent_ent.append([i[\"sentence_A\"],i[\"sentence_B\"],'ent'])\n",
    "            else:\n",
    "                sent_ent.append([i[\"sentence_B\"],i[\"sentence_A\"],'ent'])\n",
    "        elif i[\"label\"] == 1:\n",
    "            if i[\"entailment_AB\"] == ' A_neutral_B':\n",
    "#             conti\n",
    "                sent_neu.append([i[\"sentence_A\"],i[\"sentence_B\"],'neu'])\n",
    "            else:\n",
    "                sent_neu.append([i[\"sentence_B\"],i[\"sentence_A\"],'neu'])\n",
    "\n",
    "        elif i[\"label\"] == 2:\n",
    "            if i[\"entailment_AB\"] == ' A_contradicts_B':\n",
    "#             conti\n",
    "                sent_con.append([i[\"sentence_A\"],i[\"sentence_B\"],'con'])\n",
    "            else:\n",
    "                sent_con.append([i[\"sentence_B\"],i[\"sentence_A\"],'con'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sick_sent = [sent_ent[i] for i in np.random.choice(len(sent_ent)\n",
    "                                                   , 10,replace=False)]+[sent_neu[i] for i in np\n",
    "    .random.choice(len(sent_neu), 10,replace=False)]+[sent_con[i] for i in np.random.choice(len(sent_con),10,replace = False)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_sick = []\n",
    "gl_sick = []\n",
    "for i in tqdm(sick_sent):\n",
    "    try:\n",
    "        pre_data = (generate_logic(i[:-1])[-2])\n",
    "        tem = prove(pre_data,i)\n",
    "#         print(tem)\n",
    "        if tem == \"both\":\n",
    "            print(\"both\")\n",
    "            continue\n",
    "        ll_sick.append(tem)\n",
    "        gl_sick.append(i[-1])\n",
    "    except:\n",
    "        print(\"exception\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = gl_sick\n",
    "yhat_classes = ll_sick\n",
    "\n",
    "accuracy = accuracy_score(testy, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(testy, yhat_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiNLI datasets\n",
    "import json\n",
    "\n",
    "data_mnli = []\n",
    "with open('multinli_1.0_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        data_mnli.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9104b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ent = []\n",
    "sent_con = []\n",
    "sent_neu = []\n",
    "length = 10\n",
    "# with open('sentences.txt', 'w') as f:\n",
    "for i in data_mnli:\n",
    "#         if df.iloc[i,1] != \"entailment\":\n",
    "#             continue\n",
    "        if len(word_tokenize(i[\"sentence1\"]))>length or len(word_tokenize(i[\"sentence2\"]))>length:\n",
    "            continue\n",
    "\n",
    "        if i[\"gold_label\"] == \"entailment\":\n",
    "#             conti\n",
    "            sent_ent.append([i[\"sentence1\"],i[\"sentence2\"],'ent'])\n",
    "\n",
    "        elif i[\"gold_label\"] == \"contradiction\":\n",
    "            sent_con.append([i[\"sentence1\"],i[\"sentence2\"],'con'])   \n",
    "        else:\n",
    "#             elif df.iloc[i,1] == \"contradicton\":\n",
    "            sent_neu.append([i[\"sentence1\"],i[\"sentence2\"],'neu']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_sent = [sent_ent[i] for i in np.random.choice(len(sent_ent)\n",
    "                                ,10,replace=False)]+[sent_neu[i] for i in np.random.choice(len(sent_neu), 10\n",
    "                                                    ,replace=False)]+[sent_con[i] for i in np.random.choice(len(sent_con), 10\n",
    "                                                    ,replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf060d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_mn = []\n",
    "gl_mn = []\n",
    "for i in tqdm(mnli_sent):\n",
    "    try:\n",
    "        pre_data = (generate_logic(i[:-1])[-2])\n",
    "        tem = prove(pre_data,i)\n",
    "#         print(tem)\n",
    "        if tem == \"both\":\n",
    "            print(\"both\")\n",
    "            continue\n",
    "        ll_mn.append(tem)\n",
    "        gl_mn.append(i[-1])\n",
    "    except:\n",
    "        print(\"exception\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = gl_mn\n",
    "yhat_classes = ll_mn\n",
    "\n",
    "accuracy = accuracy_score(testy, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(testy, yhat_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9bc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
